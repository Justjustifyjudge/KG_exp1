"""
å°é£çŸ¥è¯†å›¾è°±åˆå§‹åŒ–å·¥å…·
ä»CSVæ–‡ä»¶æ‰¹é‡å¯¼å…¥å°é£æ•°æ®åˆ°çŸ¥è¯†å›¾è°±
"""

import pandas as pd
import numpy as np
from datetime import datetime
import logging
from typing import Dict, List, Tuple
import re

# å¯¼å…¥çŸ¥è¯†å›¾è°±æ ¸å¿ƒç±»ï¼ˆå‡è®¾åœ¨åŒç›®å½•ä¸‹ï¼‰
from knowledge_edit_sql import KnowledgeGraphCore

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class TyphoonKGInitializer:
    """å°é£çŸ¥è¯†å›¾è°±åˆå§‹åŒ–å™¨"""
    
    def __init__(self, kg: KnowledgeGraphCore):
        self.kg = kg
        self.typhoon_cache = {}  # ç¼“å­˜å·²åˆ›å»ºçš„å°é£å®ä½“
        self.region_cache = {}   # ç¼“å­˜å·²åˆ›å»ºçš„åœ°åŒºå®ä½“
    
    def load_from_csv(self, csv_path: str, sample_size: int = None) -> Dict:
        """
        ä»CSVæ–‡ä»¶åŠ è½½å°é£æ•°æ®
        
        Args:
            csv_path: CSVæ–‡ä»¶è·¯å¾„
            sample_size: é‡‡æ ·å¤§å°ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨åŠ è½½ï¼‰
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        logger.info(f"ğŸ“¥ å¼€å§‹ä»CSVåŠ è½½å°é£æ•°æ®: {csv_path}")
        
        # è¯»å–CSV
        try:
            df = pd.read_csv(csv_path, encoding='utf-8-sig')
        except:
            df = pd.read_csv(csv_path, encoding='gbk')
        
        logger.info(f"  æ€»è®°å½•æ•°: {len(df)}")
        
        # é‡‡æ ·ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if sample_size and sample_size < len(df):
            df = df.sample(n=sample_size, random_state=42)
            logger.info(f"  é‡‡æ ·åè®°å½•æ•°: {len(df)}")
        
        # æ•°æ®æ¸…æ´—
        df = self._clean_data(df)
        
        # æŒ‰å°é£ç¼–å·åˆ†ç»„
        typhoon_groups = df.groupby('å°é£ç¼–å·')
        logger.info(f"  å”¯ä¸€å°é£æ•°: {len(typhoon_groups)}")
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "total_records": len(df),
            "unique_typhoons": len(typhoon_groups),
            "entities_added": 0,
            "relations_added": 0,
            "time_start": datetime.now()
        }
        
        # å¤„ç†æ¯ä¸ªå°é£
        for typhoon_id, group in typhoon_groups:
            try:
                self._process_typhoon(typhoon_id, group, stats)
            except Exception as e:
                logger.error(f"  âœ— å¤„ç†å°é£ {typhoon_id} å¤±è´¥: {e}")
        
        stats["time_end"] = datetime.now()
        stats["duration"] = (stats["time_end"] - stats["time_start"]).total_seconds()
        
        logger.info(f"\nâœ… æ•°æ®åŠ è½½å®Œæˆ!")
        logger.info(f"  å®ä½“æ€»æ•°: {stats['entities_added']}")
        logger.info(f"  å…³ç³»æ€»æ•°: {stats['relations_added']}")
        logger.info(f"  è€—æ—¶: {stats['duration']:.2f} ç§’")
        
        return stats
    
    def _clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ•°æ®æ¸…æ´—"""
        logger.info("  ğŸ§¹ æ•°æ®æ¸…æ´—ä¸­...")
        
        # æ›¿æ¢'-'ä¸ºNaN
        df = df.replace('-', np.nan)
        
        # è½¬æ¢æ•°å€¼åˆ—
        numeric_cols = ['ç»åº¦', 'çº¬åº¦', 'å°é£ç­‰çº§', 'é£é€Ÿ', 'æ°”å‹', 'ç§»åŠ¨é€Ÿåº¦']
        for col in numeric_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # å»é™¤å®Œå…¨ç©ºç™½çš„è¡Œ
        df = df.dropna(how='all')
        
        return df
    
    def _process_typhoon(self, typhoon_id: str, group: pd.DataFrame, stats: Dict):
        """
        å¤„ç†å•ä¸ªå°é£çš„æ‰€æœ‰æ•°æ®
        
        Args:
            typhoon_id: å°é£ç¼–å·ï¼ˆå¦‚194501ï¼‰
            group: è¯¥å°é£çš„æ‰€æœ‰è®°å½•
            stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        # è·å–å°é£åŸºæœ¬ä¿¡æ¯ï¼ˆä»ç¬¬ä¸€æ¡è®°å½•ï¼‰
        first_record = group.iloc[0]
        
        # æå–å¹´ä»½
        year = int(typhoon_id[:4])
        
        # è·å–ä¸­æ–‡åç§°
        cn_name = first_record.get('å°é£ä¸­æ–‡åç§°')
        if pd.isna(cn_name) or cn_name == '-':
            cn_name = None
        
        # è·å–è‹±æ–‡åç§°
        en_name = first_record.get('å°é£è‹±æ–‡åç§°')
        if pd.isna(en_name) or en_name == '-':
            en_name = None
        
        # æ„å»ºå°é£åç§°
        if cn_name:
            typhoon_name = f"å°é£{cn_name}"
        elif en_name:
            typhoon_name = f"å°é£{en_name}"
        else:
            typhoon_name = f"å°é£{typhoon_id}"
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        max_wind_speed = group['é£é€Ÿ'].max()
        min_pressure = group['æ°”å‹'].min()
        max_intensity = self._get_max_intensity(group)
        
        # æ—¶é—´ä¿¡æ¯
        start_time = first_record.get('å°é£èµ·å§‹æ—¶é—´')
        end_time = first_record.get('å°é£ç»“æŸæ—¶é—´')
        
        # æ„å»ºå±æ€§å­—å…¸
        properties = {
            "year": year,
            "typhoon_id": typhoon_id,
            "start_time": str(start_time) if pd.notna(start_time) else None,
            "end_time": str(end_time) if pd.notna(end_time) else None,
            "record_count": len(group)
        }
        
        if cn_name:
            properties["chinese_name"] = cn_name
        if en_name:
            properties["english_name"] = en_name
        if pd.notna(max_wind_speed):
            properties["max_wind_speed"] = int(max_wind_speed)
        if pd.notna(min_pressure):
            properties["min_pressure"] = int(min_pressure)
        if max_intensity:
            properties["max_intensity"] = max_intensity
        
        # åˆ›å»ºå°é£å®ä½“
        entity_id = self.kg.add_entity_smart(
            typhoon_name,
            "å°é£",
            properties
        )
        
        if entity_id:
            stats["entities_added"] += 1
            self.typhoon_cache[typhoon_id] = entity_id
            
            # å¤„ç†è½¨è¿¹å’Œåœ°ç†å…³ç³»
            self._process_trajectory(typhoon_id, entity_id, group, stats)
    
    def _get_max_intensity(self, group: pd.DataFrame) -> str:
        """è·å–æœ€å¤§å¼ºåº¦"""
        intensity_order = [
            "è¶…å¼ºå°é£",
            "å¼ºå°é£", 
            "å°é£(TY)",
            "å¼ºçƒ­å¸¦é£æš´(STS)",
            "çƒ­å¸¦é£æš´(TS)",
            "çƒ­å¸¦ä½å‹(TD)"
        ]
        
        intensities = group['å°é£å¼ºåº¦'].dropna().unique()
        
        for intensity in intensity_order:
            if intensity in intensities:
                return intensity
        
        return None
    
    def _process_trajectory(self, typhoon_id: str, entity_id: str, 
                           group: pd.DataFrame, stats: Dict):
        """
        å¤„ç†å°é£è½¨è¿¹ï¼Œæå–åœ°ç†ä½ç½®å…³ç³»
        
        Args:
            typhoon_id: å°é£ç¼–å·
            entity_id: å°é£å®ä½“ID
            group: å°é£æ•°æ®
            stats: ç»Ÿè®¡ä¿¡æ¯
        """
        # æå–è½¨è¿¹ç‚¹ï¼ˆæ¯éš”nä¸ªç‚¹é‡‡æ ·ï¼Œé¿å…è¿‡å¤šï¼‰
        sample_interval = max(1, len(group) // 20)  # æœ€å¤š20ä¸ªè½¨è¿¹ç‚¹
        trajectory = group.iloc[::sample_interval]
        
        # åˆ¤æ–­ç™»é™†åœ°åŒºï¼ˆæ ¹æ®ç»çº¬åº¦ï¼‰
        landfall_regions = self._detect_landfall_regions(trajectory)
        
        for region in landfall_regions:
            # åˆ›å»ºåœ°åŒºå®ä½“
            region_id = self._get_or_create_region(region, stats)
            
            if region_id:
                # åˆ›å»º"ç™»é™†äº"å…³ç³»
                success = self.kg.add_relation(entity_id, "ç™»é™†äº", region_id)
                if success:
                    stats["relations_added"] += 1
    
    def _detect_landfall_regions(self, trajectory: pd.DataFrame) -> List[str]:
        """
        æ ¹æ®è½¨è¿¹åæ ‡åˆ¤æ–­ç™»é™†åœ°åŒº
        
        ç®€åŒ–ç‰ˆæœ¬ï¼šæ ¹æ®ç»çº¬åº¦èŒƒå›´åˆ¤æ–­
        """
        regions = set()
        
        # ä¸­å›½æ²¿æµ·çœä»½ç»çº¬åº¦èŒƒå›´ï¼ˆç®€åŒ–ç‰ˆï¼‰
        region_bounds = {
            "æµ·å—": {"lon": (108, 111), "lat": (18, 20)},
            "å¹¿ä¸œ": {"lon": (109, 117), "lat": (20, 25)},
            "å¹¿è¥¿": {"lon": (104, 112), "lat": (20, 26)},
            "ç¦å»º": {"lon": (115, 120), "lat": (23, 28)},
            "æµ™æ±Ÿ": {"lon": (118, 123), "lat": (27, 31)},
            "æ±Ÿè‹": {"lon": (116, 122), "lat": (30, 35)},
            "ä¸Šæµ·": {"lon": (120, 122), "lat": (30, 32)},
            "å±±ä¸œ": {"lon": (114, 123), "lat": (34, 38)},
            "å°æ¹¾": {"lon": (119, 122), "lat": (21, 26)},
        }
        
        for _, point in trajectory.iterrows():
            lon = point.get('ç»åº¦')
            lat = point.get('çº¬åº¦')
            
            if pd.isna(lon) or pd.isna(lat):
                continue
            
            # æ£€æŸ¥æ˜¯å¦åœ¨æŸä¸ªåœ°åŒºèŒƒå›´å†…
            for region, bounds in region_bounds.items():
                if (bounds["lon"][0] <= lon <= bounds["lon"][1] and 
                    bounds["lat"][0] <= lat <= bounds["lat"][1]):
                    regions.add(region)
        
        return list(regions)
    
    def _get_or_create_region(self, region_name: str, stats: Dict) -> str:
        """è·å–æˆ–åˆ›å»ºåœ°åŒºå®ä½“"""
        # æ£€æŸ¥ç¼“å­˜
        if region_name in self.region_cache:
            return self.region_cache[region_name]
        
        # åˆ›å»ºæ–°çš„åœ°åŒºå®ä½“
        entity_id = self.kg.add_entity_smart(
            region_name,
            "åœ°åŒº",
            {
                "province": region_name,
                "coastal": True
            }
        )
        
        if entity_id:
            self.region_cache[region_name] = entity_id
            stats["entities_added"] += 1
        
        return entity_id
    
    def load_with_filters(self, csv_path: str, 
                         year_start: int = None,
                         year_end: int = None,
                         has_chinese_name: bool = False) -> Dict:
        """
        å¸¦è¿‡æ»¤æ¡ä»¶çš„åŠ è½½
        
        Args:
            csv_path: CSVæ–‡ä»¶è·¯å¾„
            year_start: èµ·å§‹å¹´ä»½
            year_end: ç»“æŸå¹´ä»½
            has_chinese_name: æ˜¯å¦åªåŠ è½½æœ‰ä¸­æ–‡åçš„å°é£
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        logger.info(f"ğŸ“¥ å¼€å§‹åŠ è½½å°é£æ•°æ®ï¼ˆå¸¦è¿‡æ»¤ï¼‰")
        logger.info(f"  å¹´ä»½èŒƒå›´: {year_start or 'æ— é™åˆ¶'} - {year_end or 'æ— é™åˆ¶'}")
        logger.info(f"  åªåŠ è½½æœ‰åç§°: {has_chinese_name}")
        
        # è¯»å–CSV
        try:
            df = pd.read_csv(csv_path, encoding='utf-8-sig')
        except:
            df = pd.read_csv(csv_path, encoding='gbk')
        
        # æ•°æ®æ¸…æ´—
        df = self._clean_data(df)
        
        # è¿‡æ»¤
        if year_start or year_end:
            df['year'] = df['å°é£ç¼–å·'].astype(str).str[:4].astype(int)
            if year_start:
                df = df[df['year'] >= year_start]
            if year_end:
                df = df[df['year'] <= year_end]
        
        if has_chinese_name:
            df = df[df['å°é£ä¸­æ–‡åç§°'].notna() & (df['å°é£ä¸­æ–‡åç§°'] != '-')]
        
        logger.info(f"  è¿‡æ»¤åè®°å½•æ•°: {len(df)}")
        
        # å¤„ç†
        stats = {
            "total_records": len(df),
            "unique_typhoons": 0,
            "entities_added": 0,
            "relations_added": 0,
            "time_start": datetime.now()
        }
        
        typhoon_groups = df.groupby('å°é£ç¼–å·')
        stats["unique_typhoons"] = len(typhoon_groups)
        
        logger.info(f"  å”¯ä¸€å°é£æ•°: {stats['unique_typhoons']}")
        
        for typhoon_id, group in typhoon_groups:
            try:
                self._process_typhoon(typhoon_id, group, stats)
            except Exception as e:
                logger.error(f"  âœ— å¤„ç†å°é£ {typhoon_id} å¤±è´¥: {e}")
        
        stats["time_end"] = datetime.now()
        stats["duration"] = (stats["time_end"] - stats["time_start"]).total_seconds()
        
        logger.info(f"\nâœ… æ•°æ®åŠ è½½å®Œæˆ!")
        logger.info(f"  å®ä½“æ€»æ•°: {stats['entities_added']}")
        logger.info(f"  å…³ç³»æ€»æ•°: {stats['relations_added']}")
        logger.info(f"  è€—æ—¶: {stats['duration']:.2f} ç§’")
        
        return stats


# ============ ä½¿ç”¨ç¤ºä¾‹ ============

def initialize_full_database():
    """å®Œæ•´æ•°æ®åº“åˆå§‹åŒ–ï¼ˆå…¨é‡æ•°æ®ï¼‰"""
    print("\n" + "="*70)
    print("ğŸŒŠ å°é£çŸ¥è¯†å›¾è°± - å®Œæ•´æ•°æ®åº“åˆå§‹åŒ–")
    print("="*70)
    
    # åˆå§‹åŒ–çŸ¥è¯†å›¾è°±
    kg = KnowledgeGraphCore("typhoon_kg_full.db")
    initializer = TyphoonKGInitializer(kg)
    
    # åŠ è½½æ•°æ®
    stats = initializer.load_from_csv(
        "typhoon_data.csv"
    )
    
    print("\n" + "="*70)
    print("âœ¨ åˆå§‹åŒ–å®Œæˆï¼")
    print("="*70)
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æ€»è®°å½•æ•°: {stats['total_records']}")
    print(f"  å°é£æ•°é‡: {stats['unique_typhoons']}")
    print(f"  å®ä½“æ€»æ•°: {stats['entities_added']}")
    print(f"  å…³ç³»æ€»æ•°: {stats['relations_added']}")
    print(f"  è€—æ—¶: {stats['duration']:.2f} ç§’")


def initialize_recent_typhoons():
    """åˆå§‹åŒ–è¿‘å¹´å°é£ï¼ˆ2000å¹´è‡³ä»Šï¼‰"""
    print("\n" + "="*70)
    print("ğŸŒŠ å°é£çŸ¥è¯†å›¾è°± - åˆå§‹åŒ–è¿‘å¹´æ•°æ®ï¼ˆ2000-2024ï¼‰")
    print("="*70)
    
    # åˆå§‹åŒ–çŸ¥è¯†å›¾è°±
    kg = KnowledgeGraphCore("typhoon_kg_recent.db")
    initializer = TyphoonKGInitializer(kg)
    
    # åŠ è½½2000å¹´ä»¥åçš„æ•°æ®
    stats = initializer.load_with_filters(
        "typhoon_data.csv",
        year_start=2000,
        year_end=2024,
        has_chinese_name=True  # åªåŠ è½½æœ‰ä¸­æ–‡åçš„
    )
    
    print("\n" + "="*70)
    print("âœ¨ åˆå§‹åŒ–å®Œæˆï¼")
    print("="*70)
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æ€»è®°å½•æ•°: {stats['total_records']}")
    print(f"  å°é£æ•°é‡: {stats['unique_typhoons']}")
    print(f"  å®ä½“æ€»æ•°: {stats['entities_added']}")
    print(f"  å…³ç³»æ€»æ•°: {stats['relations_added']}")
    print(f"  è€—æ—¶: {stats['duration']:.2f} ç§’")


def initialize_sample():
    """åˆå§‹åŒ–æ ·æœ¬æ•°æ®ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰"""
    print("\n" + "="*70)
    print("ğŸŒŠ å°é£çŸ¥è¯†å›¾è°± - åˆå§‹åŒ–æ ·æœ¬æ•°æ®")
    print("="*70)
    
    # åˆå§‹åŒ–çŸ¥è¯†å›¾è°±
    kg = KnowledgeGraphCore("typhoon_kg_sample.db")
    initializer = TyphoonKGInitializer(kg)
    
    # åªåŠ è½½1000æ¡è®°å½•åšæµ‹è¯•
    stats = initializer.load_from_csv(
        "typhoon_data.csv",
        sample_size=1000
    )
    
    print("\n" + "="*70)
    print("âœ¨ åˆå§‹åŒ–å®Œæˆï¼")
    print("="*70)
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æ€»è®°å½•æ•°: {stats['total_records']}")
    print(f"  å°é£æ•°é‡: {stats['unique_typhoons']}")
    print(f"  å®ä½“æ€»æ•°: {stats['entities_added']}")
    print(f"  å…³ç³»æ€»æ•°: {stats['relations_added']}")
    print(f"  è€—æ—¶: {stats['duration']:.2f} ç§’")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        mode = sys.argv[1]
        
        if mode == "full":
            initialize_full_database()
        elif mode == "recent":
            initialize_recent_typhoons()
        elif mode == "sample":
            initialize_sample()
        else:
            print("ç”¨æ³•: python typhoon_kg_initializer.py [full|recent|sample]")
    else:
        # é»˜è®¤ä½¿ç”¨æ ·æœ¬æ¨¡å¼
        print("æç¤º: ä½¿ç”¨æ ·æœ¬æ¨¡å¼ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰")
        print("  å®Œæ•´åŠ è½½: python typhoon_kg_initializer.py full")
        print("  è¿‘å¹´æ•°æ®: python typhoon_kg_initializer.py recent")
        print("  æ ·æœ¬æ•°æ®: python typhoon_kg_initializer.py sample")
        print()
        initialize_sample()